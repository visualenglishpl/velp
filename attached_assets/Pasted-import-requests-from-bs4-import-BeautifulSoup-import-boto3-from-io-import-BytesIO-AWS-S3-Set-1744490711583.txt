import requests
from bs4 import BeautifulSoup
import boto3
from io import BytesIO

# AWS S3 Setup
s3 = boto3.client('s3', region_name='eu-north-1')
BUCKET_NAME = 'visualenglishmaterial'  # your bucket name

# Function to upload content to S3
def upload_to_s3(file_name, content):
    s3.put_object(Bucket=BUCKET_NAME, Key=file_name, Body=content)
    print(f'Uploaded {file_name} to S3.')

# List of units to scrape (Unit 1 to Unit 16)
units = [f"visualenglish5unit{i}" for i in range(1, 17)]  # Generates: visualenglish5unit1, visualenglish5unit2, ..., visualenglish5unit16

# Function to scrape and upload unit data
def scrape_and_upload(unit_slug):
    url = f"https://visualenglish.pl/product/{unit_slug}/"
    response = requests.get(url)
    
    if response.status_code != 200:
        print(f"Failed to retrieve {unit_slug}")
        return
    
    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract content (example: getting the unit title and text)
    unit_title = soup.find('h1').get_text(strip=True)  # example: get the unit title
    unit_content = soup.find('div', class_='product-content')  # modify this based on the structure of the page
    unit_text = unit_content.get_text(strip=True)  # Get the text content

    # Example: Collecting image URLs (if any)
    images = unit_content.find_all('img')
    image_urls = [img['src'] for img in images if img.get('src')]

    # Prepare the unit data (text and images)
    unit_data = {
        "title": unit_title,
        "text": unit_text,
        "images": image_urls
    }

    # Upload the text data to S3
    text_file_name = f"{unit_slug}_{unit_title}.txt"
    upload_to_s3(text_file_name, unit_data['text'])

    # Upload each image to S3
    for i, img_url in enumerate(image_urls):
        image_response = requests.get(img_url)
        if image_response.status_code == 200:
            img_name = f"{unit_slug}_{unit_title}_image_{i+1}.jpg"
            upload_to_s3(img_name, image_response.content)

    print(f"Scraping and upload for {unit_slug} complete.")

# Loop through the units (1 to 16)
for unit in units:
    scrape_and_upload(unit)

print("All units scraped and uploaded.")
